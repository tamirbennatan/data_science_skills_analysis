{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "import feedparser\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEED_URL='http://www.indeed.com/jobs?q=data+scientist&l=New+York%2C+NY' \n",
    "fp = feedparser.parse(FEED_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#method accepts a BeautifulSoup parsed HTML object and returns a list of <a> tags of job posts\n",
    "def get_job_posts(soup_object):\n",
    "    \n",
    "    job_posts = []\n",
    "    a_s = soup_object.find_all('a')\n",
    "    for a in a_s:\n",
    "        try:\n",
    "            if u'jobtitle' in a['class']:\n",
    "                job_posts.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return job_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_spaces(text):\n",
    "    \n",
    "    replace_list = ['machine learning', 'neural network', 'decision tree', 'graph database'\n",
    "               'supervised learning', 'unsupervised learning', 'reinforcement learning', \n",
    "               'logistic regression', 'linear regression', 'naive bayes', 'random forest', \n",
    "               'deep learning', 'support_vector_machines', 'advanced degree', 'computer science'\n",
    "               'ph d', 'm sc', 'b sc', 'b a' 'feature selection', 'natural language', 'sql server'\n",
    "                'operations research', 'voted best place to work', 'cloud computing', 'century link']\n",
    "    \n",
    "    for word in replace_list:\n",
    "        text.replace(word, word.replace(' ', '_'))\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method accepts a link to a job post and returns a list of \n",
    "def get_words(job_post_href):\n",
    "    \n",
    "    #only keep the content\n",
    "    try:\n",
    "        post_soup = BeautifulSoup(feedparser.parse(job_post_href)['feed']['summary'], 'html.parser')\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    text = post_soup.get_text().lower()\n",
    "    \n",
    "    # keep only letters, but keep '+' for 'c++', '#' for 'c#','3' for 'd3,\n",
    "    # '2' for 'db2', and  '-' for 'scikit-learn'\n",
    "    text = re.sub(\"[^a-z+#32-]\",\" \", text)\n",
    "    \n",
    "    #replace spaces in between important phrases with underscores so they don't get seperated as different words\n",
    "    text = replace_spaces(text)\n",
    "    \n",
    "    \n",
    "    #break into lines to get rid of the annoying '\\n' characters. Also project to lowercase.\n",
    "    lines = [line.strip().lower() for line in text.splitlines()]\n",
    "\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words += [each_word for each_word in line.split()]\n",
    "        \n",
    "    #Lighten the load by getting rid of basic stopwords, like \"the\", \"or\" etc    \n",
    "    words = set(words) - set(nltk.corpus.stopwords.words(\"english\")) \n",
    "    \n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a string with machine_learning seperated with a space. go! is also here'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'this is a string with machine learning seperated with a space. go! is also here'\n",
    "a.replace('machine learning', 'machine learning'.replace(' ' , '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method accepts a list of words in a job post, and creates sets according to pre-determined sets\n",
    "def words_by_category(job_posts):\n",
    "    \n",
    "    buzz_words = set(['big_data', 'terabytes', 'petabytes', 'voted_best_place_to_work'])\n",
    "    \n",
    "    languages = set(['r', 'python', 'java', 'scala', 'c', 'c++', 'c#', 'c--', \n",
    "                               'f', 'f#', 'groovy', 'julia', 'jscript', \n",
    "                               'matlab', 'perl', 'javascript', 'php', 'swift', \n",
    "                               'coffee', 'sql','psql', 'tsql',  'mathematica', 'wolfram', 'pascal'])#want to include 'go!' once find out how to allow '!'\n",
    "    \n",
    "    frameworks = set(['angular',  'angularjs', 'typescript' ,'backbone', 'underscore' ,\n",
    "                      'asp', 'node', 'django', 'flask' ])\n",
    "    \n",
    "    hadoop_technologies = set(['hadoop', 'pig', 'mapreduce', 'spark', 'hive', 'flume', \n",
    "                              'shark', 'zookeeper', 'mahout' ])\n",
    "\n",
    "    #operating systems and tools\n",
    "    operating_system_tools = set(['unix', 'osx', 'bash', 'batch' , 'curl', 'linux', 'windows', 'git' ])\n",
    "    \n",
    "    cloud_computing = set(['cloud_computing', 'aws', 'azure', 'cloudera', 'chef', 'joyent', 'rackspace', \n",
    "                           'century_link' ])\n",
    "        \n",
    "    machine_learning_algorithms = set(['decision_trees', 'clustering', 'regression', 'anova', \n",
    "                                       'k-means', 'neural_networks', 'supervised learning', 'unsupervised_learning',\n",
    "                                       'reinforcement_learning', 'logistic_regression', 'linear_regression', 'naive_bayes', 'random_forest', \n",
    "                                       'deep_learning', 'pca', 'shrinkage', 'regression', 'support_vector_machines', 'svm', \n",
    "                                      'boosted', 'feature_selection', 'natural_language', 'nlp', 'cart', ])\n",
    "    \n",
    "    analysis_tools = set(['excel', 'sas', 'spss', 'sas', 'tableau', 'visual_basic' ])\n",
    "    \n",
    "    relational_databases = set(['redshift', 'postgressql', 'mysql', 'oracle', 'db2', 'h2', \n",
    "                               'sqlbase', 'libreoffice', 'netezza', 'azure', 'firebird', 'sql_server'])\n",
    "    \n",
    "    nosql_technologies = set(['nosql', 'hbase', 'cloudera', 'cassandra', 'scylla', 'mongodb', 'sonarw', 'jsonar', \n",
    "                             'elassandra', 'couchdb', 'rethinkdb', 'dynamodb', 'arangodb'])\n",
    "    \n",
    "    graph_databases = set(['graph_databases', 'graph_database', 'graph_databases', 'neo4j', 'arangodb', 'orientdb', 'graphbase', 'trinity'])\n",
    "    \n",
    "    academic_degrees = set(['bachelor', 'master','doctor', 'doctorate', 'ph_d', 'phd', \n",
    "                            'm_sc', 'b_a', 'b_sc', 'advanced_degree', 'mba'])\n",
    "    \n",
    "    academic_disciplines = set(['mathematics', 'statistics', 'computer_science', 'business', 'stem', \n",
    "                               'operations_research'])\n",
    "    \n",
    "    \n",
    "    #for each job post see which skills appear. Leave as python.set data strucuture for effecient set operations later\n",
    "    for post in job_posts:\n",
    "        post['buzz_words'] = set(post['words']) & buzz_words\n",
    "        post['languages'] = set(post['words']) & languages\n",
    "        post['frameworks'] = set(post['words']) & frameworks\n",
    "        post['hadoop_technologies'] = set(post['words']) & hadoop_technologies\n",
    "        post['operating_system_tools'] = set(post['words']) & operating_system_tools\n",
    "        post['machine_learning_algorithms'] = set(post['words']) & machine_learning_algorithms\n",
    "        post['analysis_tools'] = set(post['words']) & analysis_tools\n",
    "        post['relational_databases'] = set(post['words']) & relational_databases\n",
    "        post['nosql_technologies'] = set(post['words']) & nosql_technologies\n",
    "        post['graph_databases'] = set(post['words']) & graph_databases\n",
    "        post['academic_degrees'] = set(post['words']) & academic_degrees\n",
    "        post['academic_disciplines'] = set(post['words']) & academic_disciplines\n",
    "        post['academic_degrees'] = set(post['words']) & academic_degrees\n",
    "        \n",
    "        post['all_skills'] = buzz_words.union(languages).union(frameworks).union(\n",
    "            hadoop_technologies).union(operating_system_tools).union(machine_learning_algorithms).union(\n",
    "            analysis_tools).union(relational_databases).union(nosql_technologies).union(graph_databases).union(\n",
    "            academic_degrees).union(academic_disciplines) & set(post['words'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 4, 5, 12}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,2,4]) and set([2,12, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawl_indeed(query, city = \"\", experience_level = \"\", num_pages = 10):\n",
    "    \n",
    "    #clean parameters so that they match Indeed's protocol\n",
    "    \n",
    "    #spaces parameters seperated by '+'\n",
    "    query = query.replace(' ', '+')\n",
    "    city = city.replace(' ' , '+')\n",
    "    \n",
    "    #make sure experience_level is one of the four valid options\n",
    "    if experience_level not in ['', 'entry_level', 'mid_level', 'senior_level']:\n",
    "        experience_level = ''\n",
    "        print \"Experience level parameter not valid. Showing all experience levels\"\n",
    "    \n",
    "    #build data in JSON-like format, given the heirarchial nature of the data\n",
    "    data = list()\n",
    "    \n",
    "    #Indeed shows job posts 10 at a time, so each page starts with post 0, 10, 20...\n",
    "    #Loop through the posts using the num_pages parameter\n",
    "    \n",
    "    page_start_numbers = np.arange(num_pages)*10\n",
    "    \n",
    "    for start_number in page_start_numbers:\n",
    "        \n",
    "        url = \"http://www.indeed.com/jobs?q={0}&l={1}&explvl={2}&start={3}\".format(query, city, experience_level, start_number)\n",
    "    \n",
    "        #isolate html body and get rid of extraneous HTML objects using feedparser, and create a BeautifulSoup obect\n",
    "        fp = feedparser.parse(url)\n",
    "        try:\n",
    "            page_soup = BeautifulSoup(fp['feed']['summary'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #get a list of all the <a class='jobtitle'></a> elemdents, which are job posts\n",
    "        job_posts = get_job_posts(page_soup)\n",
    "\n",
    "        for post in job_posts:\n",
    "            \n",
    "            #each job post gets a sub-dictionary\n",
    "            post_data = dict()\n",
    "            \n",
    "            #attributes which are constant amongst all results for each execution\n",
    "            post_data['query'] = query\n",
    "            post_data['city'] = city\n",
    "            \n",
    "            #job title of that sepcific post\n",
    "            post_data['title'] = post['title']\n",
    "            \n",
    "            \n",
    "            #for each job post, extract the link to the post itself. Store in data dictionary for later\n",
    "            post_href = post['href']\n",
    "            post_data['href'] = post_href\n",
    "            \n",
    "            #extract all the (cleaned up) words fromt that link\n",
    "            post_words = get_words(post_href)\n",
    "                        \n",
    "            post_data['words'] =post_words\n",
    "            data.append(post_data)\n",
    "            \n",
    "    words_by_category(data)\n",
    "    return data\n",
    "            \n",
    "            \n",
    "    #http://www.indeed.com/jobs?q=data+scientist&l=New+York,+NY&explvl=entry_level\n",
    "    #http://www.indeed.com/jobs?q=data+scientist&l=New+York,+NY&explvl=senior_level&start=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_scientist_nyc = crawl_indeed('data scientist', 'new york', num_pages = 15)\n",
    "data_scientist_sanfran = crawl_indeed('data scientist', 'san fransisco', num_pages = 15)\n",
    "data_scientist_chicago = crawl_indeed('data scientist', 'san fransisco', num_pages = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_scientist_all = crawl_indeed('data scientist', num_pages = 5)\n",
    "data_analyst_all = crawl_indeed('data analyst', num_pages = 15)\n",
    "data_engineer_all = crawl_indeed('data engineer', num_pages = 15)\n",
    "machine_learning_engineer_all = crawl_indeed('machine learning engineer', num_pages = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([1, 2, 4, 6]) set([2, 3, 4, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2, 4, 6}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print a , b\n",
    "\n",
    "a & b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'city', u'high_level_languages', u'low_level_languages', u'query',\n",
       "       u'title', u'words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data2).iloc[9].index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
